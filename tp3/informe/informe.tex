% ------ headers globales -------------
\documentclass[11pt, a4paper, twoside]{article}
\usepackage{header}
\usepackage{config}
% -------------------------------------
\begin{document}

% -- Carátula --
\clearpage{\pagestyle{empty}\input{caratula}}

\newpage
\input{introduccion}

%-- Índice --
\clearpage{%
  \pagestyle{empty}\tableofcontents%
  \vspace{3cm}%
  %\begin{abstract}
  %\end{abstract}
  \clearpage%
}
%-- A partir de aquí, pongo el contador de páginas en 1 --
\setcounter{page}{1}

\section{Implementación de Map-Reduce}

\subsection{Encontrar el subreddit con mayor score promedio}


\begin{figure}[H]
  \caption{La función \texttt{map}}
  \centering
\begin{verbatim}
function () {
    emit(this.subreddit, { count: 1, sum: this.score} );
}
\end{verbatim}

\end{figure}

La función $map$ se aplica a cada post y emite, con el $subreddit$ como key, una tupla con la cantidad de posts - inicializada en 1 - y la suma de los scores -- inicializada con el score del post.

\begin{figure}[H]
\caption{La función \texttt{reduce}}
\centering
\begin{verbatim}
function (key, values) {
    reducedVal = {count: 0, sum: 0};
    for (var i = 0; i < values.length; i++) {
        reducedVal.count += values[i].count;
        reducedVal.sum += values[i].sum;
    };
    return reducedVal;
}
\end{verbatim}
\end{figure}

Tras aplicar la función $map$ quedan asociados cada subreddit con una lista de las tuplas asociados a los posts del subreddit. La función $reduce$ toma un fragmento de esta lista y la reduce en una sola tupla que acumula la cantidad de posts y la suma de los scores.


\begin{figure}[H]
\caption{La función \texttt{finalize}}
\centering
\begin{verbatim}
function finalize (key, reducedVal) {
    return reducedVal.sum / reducedVal.count;
}
\end{verbatim}
\end{figure}

Luego de la etapa de $reduce$, quedan asociados cada subreddit con su cantidad de posts y su suma de scores. La función $finalize$ calcula el promedio final realizando una división.

Ejecutamos el query y utilizamos la función \texttt{sort} en la consola de Mongo. El subreddit $GirlGamers$ tiene el mayor score promedio: 2483.

\newpage
\subsection{Encontrar los doce títulos con mayor score de la colección de posts con al menos 2000 votos.}

\begin{figure}[H]
\caption{La función \texttt{map}}
\centering
\begin{verbatim}
function () {
    if (this.total_votes >= 2000)
        emit(this.title, this.total_votes);
}
\end{verbatim}
\end{figure}

Se emiten, para todos los posts con al menos 2000 votos, su título y su cantidad de votos.

\begin{figure}[H]
\caption{La función \texttt{reduce}}
\centering
\begin{verbatim}
function (key, values) {
    return Array.sum(values);
}
\end{verbatim}
\end{figure}

Para cada título, se suman los votos de sus posts.

Tras ejecutar el query, se ordenaron los resultados. Los doce títulos con mayor score de la colección de posts con al menos 2000 votos son:

\begin{center}
  \begin{tabular}{l|c}
    %\hline
    \textbf{Título} & \textbf{Score} \\ \hline
    ``Airline screwed up, a friend just posted this on Facebook.'' & 177103 \\
    ``Following the Obama AMA'' & 144145 \\
    ``Nailed it.'' & 129307 \\
    ``Help a brother out!'' & 129183 \\
    ``Seen in NJ, what a friendly neighbor'' & 126222 \\
    ``McKayla Maroney visits the White House... her picture with the President'' & 120929 \\
    ``The Bus Knight'' & 119774 \\
    ``Brilliant and thoughtful parents handed these out to everyone on my flight.'' & 119232 \\
    ``Screenshot of reddit from the year 3012'' & 118866 \\
    ``Standing guard, hurricane or otherwise'' & 117272 \\
    ``My neighbors are taking this especially hard.'' & 117101 \\
    ``When I found out I could upvote by pressing 'A''' & 115717

  \end{tabular}
\end{center}

\newpage
\subsection{Para los diez mejores scores, calcular la cantidad de comentarios en promedio por sumisión.}

Este ejercicio se resuelve de manera análoga al ejercicio 1.

\begin{figure}[H]
\caption{La función \texttt{map}}
\centering
\begin{verbatim}
function () {
    emit(this.score, {count: 1, sum: this.number_of_comments});
}
\end{verbatim}
\end{figure}

\begin{figure}[H]
\caption{La función \texttt{reduce}}
\centering
\begin{verbatim}
function (key, values) {
    reducedVal = {count: 0, sum: 0};
    for (var i = 0; i < values.length; i++) {
        reducedVal.count += values[i].count;
        reducedVal.sum += values[i].sum;
    };
    return reducedVal;
}
\end{verbatim}
\end{figure}

\begin{figure}[H]
\caption{La función \texttt{finalize}}
\centering
\begin{verbatim}
function finalize (key, reducedVal) {
    return reducedVal.sum / reducedVal.count;
}
\end{verbatim}
\end{figure}

Los resultados se ordenaron. Los diez mejores scores y la cantidad de comentarios en promedio por sumisión son:

\begin{center}
  \begin{tabular}{l|c}
    %\hline
    \textbf{Score} & \textbf{Comentarios en promedio} \\ \hline
    20570 & 1463 \\
    12333 & 1612 \\
    11908 & 2681 \\
    10262 & 1514 \\
    8935 & 480 \\
    8835 & 1716 \\
    8699 & 934 \\
    8241 & 571 \\
    7297 & 1110 \\
    6741 & 2204
  \end{tabular}
\end{center}

\subsection{Entre los usuarios con a la sumo 5 sumisiones, encontrar el que posea mayor cantidad de upvotes.}

Este ejercicio se resuelve de forma similar al ejercicio 1, salvo que en la función $finalize$ se filtran los usuarios con más de 5 sumisiones.

\begin{figure}[H]
\caption{La función \texttt{map}}
\centering
\begin{verbatim}
function () {
    emit(this.username, {sumisiones: 1, number_of_upvotes: this.number_of_upvotes});
}
\end{verbatim}
\end{figure}

\begin{figure}[H]
\caption{La función \texttt{reduce}}
\centering
\begin{verbatim}
function (key, values) {
    var res = {sumisiones: 0, number_of_upvotes: 0};
    for (var i = 0; i < values.length; i++) {
        var val = values[i];
        res.sumisiones += val.sumisiones;
        res.number_of_upvotes += val.number_of_upvotes;
    }
    return res;
}
\end{verbatim}
\end{figure}

\begin{figure}[H]
\caption{La función \texttt{finalize}}
\centering
\begin{verbatim}
function finalize (key, reducedVal) {
    if (reducedVal.sumisiones <= 5)
        return reducedVal.number_of_upvotes;
}
\end{verbatim}
\end{figure}

Los resultados se ordenaron. El usuario con a lo sumo cinco sumisiones ya la mayor cantidad de upvotes es ``lepry'', con 90396 upvotes.

\newpage
\subsection{Para todos los subrredit que poseen un score entre 280 y 300, indicar la cantidad palabras
presentes nnen sus títulos}

Este ejercicio se resuelve de forma similar al ejercicio 1, salvo que en la función $finalize$ se filtran los subreddit cuyo score no se encuentra en el intervalo [280,300].

\begin{figure}[H]
\caption{La función \texttt{map}}
\centering
\begin{verbatim}
function () {
    var cantPalabras = this.title.split(' ').length;
    emit(this.subreddit, {score: this.score, cantPalabras: cantPalabras});
}
\end{verbatim}
\end{figure}

\begin{figure}[H]
\caption{La función \texttt{reduce}}
\centering
\begin{verbatim}
function (key, values) {
    var res = {score: 0, cantPalabras: 0};
    for (var i = 0; i < values.length; i++) {
        var val = values[i];
        res.score += val.score;
        res.cantPalabras += val.cantPalabras;
    }
    return res;
}
\end{verbatim}
\end{figure}

\begin{figure}[H]
\caption{La función \texttt{finalize}}
\centering
\begin{verbatim}
function finalize (key, reducedVal) {
    if (280 <= reducedVal.score && reducedVal.score <= 300)
        return reducedVal.cantPalabras;
}
\end{verbatim}
\end{figure}


Los únicos resultados arrojados fueron:

\begin{center}
  \begin{tabular}{l|c}
    %\hline
    \textbf{Subreddit} & \textbf{Palabras en títulos} \\ \hline
    ``Firearms'' & 24 \\
    ``anime'' & 18 \\
    ``Sexy'' & 13 \\
    ``Feminism'' & 6 \\
    ``HeroesofNewerth'' & 6 \\
    ``TheRealZachAnner'' & 4 \\
    ``ragecomics'' & 4 \\
    ``xkcd'' & 4 \\
  \end{tabular}
\end{center}

\clearpage
\section{Investigación}
Para esta parte del TP se pide leer e interpretar el paper <<\textbf{Job
Scheduling for Multi- User MapReduce
Clusters}>>\footnote{http://www.icsi.berkeley.edu/pubs/techreport
s/ICSI\_jobschedulingfor09.pdf}. Se sugiere además una serie de puntos
para analizar el contenido del paper. Estos son los siguientes:

\vspace{3em}
\ref{investigacion-1} Explique cuál es el entorno y la situación donde se plantea el problema. (Motivación)

\ref{investigacion-2} Identifique de problema

\ref{investigacion-3} Identifique situaciones donde el problema se dispara y la consecuencias que esto genera.

\ref{investigacion-4} Comente el background histórico del problema

\ref{investigacion-5} Explique otros problemas asociados a la búsqueda de una mejor solución

\ref{investigacion-6} Explique las soluciones propuestas

\ref{investigacion-7} Evalúe soluciones propuestas

\ref{investigacion-8} Discusión

\ref{investigacion-9} Conclusiones

\clearpage
\subsection {\footnotesize Explique cuál es el entorno y la situación donde se plantea el problema. (Motivación)}
\label{investigacion-1}

La idea tras este paper surgió luego de que los autores se vieran frente a la
tarea de diseñar un scheduler para MapReduce, para la empresa Facebook. Este
sería el encargado de repartir el poder de cómputo en un \emph{warehouse
Hadoop}\footnote{Hadoop es la versión \emph{open-source} de MapReduce.} de 600
nodos, en un entorno multiusuario, en donde se combinarían tareas de producción
con tareas experimentales.

Este warehouse era utilizado para aproximadamente 3200 operaciones de MapReduce
diarias, siendo que algunas de ellas eran tareas frecuentes de mantenimiento,
análisis de datos, antispam y optimización, cuya ejecución se prolongaba a lo
largo de horas, mientras que otras eran queries \emph{AD-HOC} cuya ejecución
demoraba unos pocos minutos. Por tal motivo, era de vital importancia poder
lograr un mecanismo de \textbf{scheduling justo}.

\clearpage
\subsection {\footnotesize Identifique el problema}
\label{investigacion-2}

Se describen principalmente dos aspectos en donde el scheduling de un cluster
MapReduce se diferencia de un mecanismo genérico de scheduling, y por los cuales
el mecanismo de scheduling utilizado previamente presentaba una pérdida de
rendimiento de entre 2 y 10 veces en comparación con el mecanismo presentado en
el paper. Los dos principales aspectos problemáticos fueron la \emph{localidad
de datos}, y la \emph{interdependencia entre las operaciones Map y Reduce}.

\subsubsection{Localidad de Datos:}

Este problema comprende la necesidad inherente de la técnica MapReduce de tener
acceso local a los datos con los cuales se están trabajando. Esta necesidad se
debe principalmente a que a diferencia de los algoritmos ``\texttt{CPU-
INTENSIVE}''\footnote{Los que necesitan un gran poder de cómputo.}, el MapReduce
entra en la categoría de los denominados ``\texttt{DATA-INTENSIVE}'', es decir,
aquellos en donde el poder de cómputo necesario es despreciable frente a la gran
cantidad de datos con los que se trabaja.

Desde esta perspectiva, contar con un scheduler eficiente en el acceso y la
utilización de los datos, es mucho más importante que en otro tipo de
situaciones. Todo esto está agravado por el costo que supone el almacenamiento,
y la transmisión de datos entre distintos servidores. Este defecto se puede
observar principalmente en \textbf{trabajos pequeños y/o concurrentes}.


\subsubsection{Interdependencia:}

Se entiende por interdependencia a la necesidad de que todas las tareas de
\texttt{map} se encuentren finalizadas al momento de iniciar la ejecución de
\texttt{reduce}. Es fácil percibir la gran cantidad de inconvenientes que esto
genera. Se mencionan particularmente los casos de \textbf{infrautilización de
los recursos y \texttt{starvation}}, en donde se da que una única tarea adquiere
cierta cantidad de slots, destinados a realizar la operación \texttt{reduce},
pero que se ven bloqueados/inutilizados hasta el momento en que esta tarea
termine de realizar su operación de \texttt{map}, evitando de esta forma que
otras tareas se ejecuten, y de \textbf{consumo excesivo de espacio de disco}
destinado a los datos intermedios producidos por \texttt{map} (datos que no
pueden ser liberados hasta que el trabajo termine).

\clearpage
\subsection {\footnotesize Identifique situaciones donde el problema se dispara y la consecuencias que esto genera.}
\label{investigacion-3}

Los problemas descriptos anteriormente se ponían en evidencia al momento de
querer realizar queries \texttt{ad-hoc} de muy corta duración frente a los
trabajos de producción que el warehouse debe correr periódicamente. Bajo esta
perspectiva, era deseable que los trabajos experimentales puedan ser lanzados
en cualquier momento, y tener un tiempo de respuesta \textbf{aceptable}. Con
la implementación del Scheduler FIFO de Hadoop, esto se volvía imposible,
reduciendo significativamente la utilidad del sistema.

\clearpage

\subsection {\footnotesize Comente el background histórico del problema}
\label{investigacion-4}
Hadoop está inspirado en el MapReduce de Google, por lo que gran parte de su
implementación básica está fuertemente ligada a este proyecto. La primer
solución al conflicto del scheduling, se encuentra provista por defecto dentro
la propia implementación de Hadoop, y consta de una cola \texttt{FIFO} de 5
niveles de prioridad, de forma tal que cada vez que un slot se libera, el
scheduler lo asigna a la más prioritaria de entre las tareas pendientes.

Sobre este enfoque, Hadoop aplica una \texttt{optimización de localidad}, al
igual que lo hace MapReduce de Google: dado que una tarea generalmente consta de
múltiples operaciones de \texttt{map}, luego de elegir una tarea el scheduler
selecciona las operaciones map más adecuadas según un \textbf{criterio de
localidad}. Serán elegidos entonces los maps que utilicen datos que se
encuentren más cerca físicamente al worker que está corriendo la tarea. Es
decir, serán elegidas primero las que se encuentren en ese mismo worker, luego
las que se encuentren en el mismo rack, y finalmente las que se encuentren en
racks remotos.

La gran desventaja del scheduler \texttt{FIFO} es el pésimo tiempo de respuesta
para trabajos pequeños cuando se encuentran intercalados con trabajos grandes.
Por ello, la primer solución que se implementó, es el mecanismo denominado
\textbf{Hadoop On Demand (HOD)}. Sin embargo, este mecanismo también ocasionó
problemas, principalmente \textbf{baja localidad de los datos} y \textbf{baja
utilización de los recursos},

\clearpage
\subsection {\footnotesize Explique otros problemas asociados a la búsqueda de una mejor solución}
\label{investigacion-5}

Para explicar el problema, es necesario ahondar un poco en los conceptos
utilizados en el paper. Se describe un \texttt{job} como un conjunto de
\texttt{tasks}, siendo un task una operación \texttt{map-reduce}. Cada nodo
(también llamados \texttt{slaves} o \texttt{workers}) tiene una determinada
cantidad de \texttt{slots de ejecución}, y conforme estos se van liberando el
Scheduler le va asignando \texttt{tasks}, siendo que cada uno de estos ocupa
un \texttt{slot}.

\centerbf{Problemas asociados a la Localidad de Datos}

% - HOL Scheduling

\subsubsection{Head-of-line scheduling}: 

Para explicar este concepto, el paper propone
una métrica, \textbf{el porcentaje de localidad\footnote{En el paper se
distingue entre localidad de nodo y localidad de rack; la idea sigue siendo la
misma.} de un job en relación a su tamaño}. Utilizando esta métrica, dado un
\texttt{job}, su porcentaje de localidad se ve representado por la cantidad de
\texttt{nodos} en la que se encuentran distribuídos los datos de sus
\texttt{tasks}, en relación al total. Se hace notar que el \texttt{porcentaje
de localidad de un job} crece en relación a su tamaño (medido en cantidad de
\texttt{tasks}), de forma tal que un \texttt{job} pequeño tiene mucha menos
\texttt{localidad} que un \texttt{job} grande. Por otro lado, dado que se
utiliza una cola \texttt{FIFO}, el siguiente \texttt{job} a ejecutar se
encuentra de cierta forma \emph{predeterminado}, por lo que no es posible
``elegir el más apropiado''.

Asi, dado un \texttt{nodo} cualquiera al que se le hayan liberado uno o más
\texttt{slots}, la probabilidad de que el siguiente \texttt{job} contenga un
\texttt{task} cuyos datos se encuentren en ese \texttt{nodo}, es proporcional al
tamaño del \texttt{job}. Y esto ocasiona problemas en los \texttt{jobs}
pequeños, los cuales suelen ser justamente los que más necesidad de
\textbf{interactividad} tienen.

% - Sticky slots

\subsubsection{Sticky Slots}:

Este problema puede aparecer incluso con \texttt{jobs} de tamaño grande. Se
puede entender con el siguiente ejemplo: Tenemos 10 \texttt{jobs} con 10
\texttt{tasks} corriendo cada uno. En cuanto un \texttt{job} termina un
\texttt{task}, se debe determinar a quién asignar el \texttt{slot libre}. Como
este mismo \texttt{job} tiene 9 \texttt{tasks} corriendo, y el resto 10, según
el \texttt{fair scheduling} se le asignaría este \texttt{slot} al mismo que lo
liberó. Como consecuencia de este comportamiento, los \texttt{jobs} difícilmente
abandonan sus \texttt{slots} originales. Entonces, si inicialmente este tenía
baja \texttt{localidad}, la sigue manteniendo hasta el final.

\centerbf{Problemas asociados a la Interdependencia de Map-Reduce}

\subsubsection{Reduce Slot Hoarding:}

Este problema está relacionado con la operación de \texttt{Reduce}. Para
entenderlo, es necesario recordar brevemente qué es lo que hace una operación
\texttt{Reduce}. Se compone básicamente de dos partes: Durante la primera fase,
en donde predomina la característica antes presentada como \texttt{DATA-
INTENSIVE}\footnote{Alta tasa de uso de Input/Output.}, se copian los datos
emitidos por la función \texttt{Map}. Luego, en una segunda fase, caracterizada
por ser \texttt{CPU-INTENSIVE}\footnote{Alta tasa de uso de CPU.}, se los
procesan.

También vale la pena mencionar que en el paper se hace distinción entre
\texttt{Slot de Map} y \texttt{Slot de Reduce}, y que estos últimos se liberan
sólamente cuando el \texttt{job} termina.

Todo esto ocasiona principalmente dos complicaciones. Primero, una mala
utilización de los \texttt{slots}: cuando un \texttt{job} se compone de miles de
\texttt{tasks}, los \texttt{map} se van ejecutando a medida que se van liberando
\texttt{slots de Map} en los \texttt{nodos}. Así, se va produciendo el output
necesario para ser utilizado por \texttt{Reduce}. De esta forma, se comienzan a
reservar slots para \texttt{Reduce}, los cuales serán ``brevemente'' utilizados
para procesar los output de los map (se procesa un output por slot), y luego
quedarán bloqueados hasta el momento en que el \texttt{job} finalice.

La segunda complicación tiene que ver con la mala administración de los recursos
dentro de la misma operación de \texttt{Reduce}. Dado que la operación se
compone de dos partes, en donde una hace un uso intensivo de los recursos de
entrada/salida, y la otra hace un uso intensivo de la CPU, y que estos se
producen en forma secuencial. Es fácil ver que en todo momento, en cada slot de
\texttt{Reduce}, aun asumiendo una buena utilización desde el punto de vista del
Scheduling, se va a estar realizando una operación de, o \texttt{Input/Ouput}, o
\texttt{CPU}, una por vez, cuando en un escenario más eficiente se podrían estar
utilizando ambas cosas a la vez.

\subsubsection{Uso de Espacio en Disco:}

Relacionado al problema descripto anteriormente, existe también un inconveniente
que se presenta al acaparar muchos \texttt{slots de Reduce} para un
\texttt{job}. Dado que el Reduce utiliza y produce \texttt{datos intermedios},
que son guardados en el disco, y que no son liberados hasta que todo el
\texttt{job} termina, esto puede llevar a producir un \texttt{acaparamiento de
disco}. En el peor caso, por ejemplo cuando hubiese muchos \texttt{jobs}
corriendo al mismo tiempo, este se podría llenar, y para solucionar habría que
descartar todos los datos de uno o varios \texttt{jobs}.

\clearpage
\subsection {\footnotesize Explique las soluciones propuestas}
\label{investigacion-6}

Para solucionar todos los inconvenientes descriptos, el paper propone un
mecanismo de Scheduling que autodenominan \texttt{FAIR}. Este mecanismo está
ideado para cumplir principalmente con dos características:

\begin{itemize}
    \item \textbf{Aislamiento}: brindarle a cada usuario la ilusión de estar
    corriendo un cluster privado.
    \item \textbf{Multiplexado Estadístico}: redistribuír en tiempo de ejecución
    los recursos no utilizados.
\end{itemize}

Para lograr estos objetivos, se implementaron dos mecanismos:

\subsubsection{Delay Scheduling:}
para solucionar los \texttt{problemas de localidad de
datos}. Para ello, cuando un \texttt{job} se encuentra en condiciones de que le
sean asignados \texttt{slots}, el Scheduler se fija si este contiene
\texttt{tasks} aptas para correr localmente en alguno de los \texttt{nodos} que
tienen \texttt{slots disponibles}. En el caso en que esto no suceda, se lo
saltea hasta encontrar algún \texttt{job} que contenta \texttt{tasks}
apropiados. Para evitar \textbf{starvation}, se implementa un mecanismo de
\textbf{timeout} mediante el cual, luego de transcurrido ese tiempo sin que le
sean asignados \texttt{slots libres}, a un \texttt{job} se le permite correr
tareas, aun cuando estas sean en forma no-local.

\subsubsection{Copy-Compute Splitting:}
para solucionar los \texttt{problemas de
interdependencia}. Se propone dividir la operación de \texttt{Reduce} en dos
tipos de \textbf{tareas independientes}, \texttt{tareas de copia} y
\texttt{tareas de cómputo}. Para ello es posible dividir las tareas en dos
procesos separados, pero esto traería muchas complicaciones desde el punto de
vista del aislamiento de los procesos, ya que sería necesario en tal caso
implementar un complejo mecanismo de \textbf{memoria compartida}. Se propone en
cambio una solución que denominan \texttt{CPAC} (\texttt{Compute Phase Admission
Control}), que básicamente se trata de reservar una cantidad de \texttt{slots de
Reduce} mayor a la que el sistema soporta, llevando cuenta en todo momento de
cuántos de estos están realizando operaciones de copia y cuántos están
realizando cómputo. Asimismo, se limita la cantidad de slots que tienen permiso
de copia y permiso de cómputo. Así, cada vez que una operación de
\texttt{Reduce} requiere hacer copia o cómputo, primero le pide permiso al
Scheduler. Así, se busca asegurar que, siendo $N_{min}$ la cantidad de slots de
Reduce que soporta el nodo en cuestión, en todo momento haya a lo sumo $N_{min}$
operaciones de copia y $N_{min}$ operaciones de cómputo corriendo
concurrentemente.

\clearpage
\subsection {\footnotesize Evalúe soluciones propuestas}
\label{investigacion-7}

Se evaluaron los algoritmos en un caso de benchmark grande, del tamaño de las tareas de producción en Facebook. El entorno principal de evaluación fue Amazon's Elastic Compute Cloud\footnote{http://aws.amazon.com/ec2/}.

Se pudo ver que el fair sharing disminuye a la mitad el tiempo de respuesta promedio comparado con FIFO en jobs pequeños, a expensas de un pequeño aumento frente a jobs grandes. Sin embargo, algunos jobs todavía eran demorados por el $reduce\ slot\ hoarding$. Al introducir el copy-compute splitting, el fair sharing obtuvo una mejora de hasta 4.6x en jobs pequeños.

El $delay\ scheduling$ permitió pasar de un porcentaje previo de localidad de 27-90\% a un asombroso nivel de 99-100\%. Para una cantidad de 50 tareas concurrentes, aumentó el throughput por un factor de $2.0$. Por otra parte, el tiempo de respuesta no presentó mejoras significativas.

\clearpage
\subsection {\footnotesize Discusión}
\label{investigacion-8}

El trabajo desarrollado en el paper consiste en encontrar un punto
intermedio óptimo entre tener un cluster separado para cada usuario (
que provee gran aislamiento pero una pobre utilización) y tener un único
cluster FIFO (que provee gran utilización pero ningún tipo de
aislamiento entre usuarios).

Se identificaron dos aspectos en el manejo de un cluster de datos
intensivo que generan problemas: localidad de datos e interdependencia
entre tareas. A pesar de que hasta aquí  todo giró en torno a MapReduce, estos
problemas son importantes en cualquier cluster de cómputos que utilize
grafos dirigidos aciclicos de procesos para efectuar cómputos, ya que este
tipo de sistema necesita colocar tareas en nodos que contienen sus inputs, y
así correr el riesgo de encontrarse con problemas de los cuales ya hablamos,
como por ejemplo \texttt{sticky slot}  y \texttt{head-of-line scheduling}.

\clearpage
\subsection {\footnotesize Conclusiones}
\label{investigacion-9}

A pesar de que MapReduce ha sido un modelo de ejecución muy popular y exitoso
para largos \texttt{batch jobs}, muchas empresas han comenzado a compartir
sus clusters de MapReduce entre múltiples usuarios, corriendo así una mezcla
de trabajos \texttt{batch} e \texttt{interactivos}. En el paper se propone
FAIR, un scheduler justo que permite aislamiento entre usuarios, bajo tiempo
de respuesta y alto \texttt{throughput}. Para lidiar con los problemas que este
scheduling genera (localidad de datos e interdependencia entre tareas),
se pueden emplear dos técnicas simples y robustas: delay scheduling y
copy-compute splitting.

\clearpage

\end{document}
